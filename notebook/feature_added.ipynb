{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we add many features base on `analysis.ipynb` and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = pd.read_csv('input/train.csv')\n",
    "titanic_test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([titanic_train, titanic_test], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the less frequently appeared NaN with mean and mode\n",
    "data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "index_NaN_age = data['Age'][data['Age'].isnull()].index.values\n",
    "age_med = data['Age'].median()\n",
    "for i in index_NaN_age:\n",
    "    age_pred = data['Age'][((data['SibSp'] == data.iloc[i]['SibSp']) \n",
    "                            & (data['Parch'] == data.iloc[i]['Parch']) \n",
    "                            & (data['Pclass'] == data.iloc[i]['Pclass']))].median()\n",
    "    if not np.isnan(age_pred):\n",
    "        data['Age'].loc[i] = age_pred\n",
    "    else:\n",
    "        data['Age'].loc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Title feature from Name\n",
    "data_title = [i.split(',')[1].split('.')[0].strip() for i in data['Name']]\n",
    "data['Title'] = pd.Series(data_title)\n",
    "data[\"Title\"] = data[\"Title\"].replace(['Lady', 'the Countess',\n",
    "                                       'Countess','Capt', 'Col','Don',\n",
    "                                       'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer',\n",
    "                                       'Dona'], 'Rare')\n",
    "data[\"Title\"] = data[\"Title\"].map({\"Master\":0,\n",
    "                                   \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1,\n",
    "                                   \"Mr\":2, \"Rare\":3})\n",
    "data[\"Title\"] = data[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the family-size feature\n",
    "data['Fsize'] = data['Parch'] + data['SibSp'] + 1\n",
    "\n",
    "# Create new feature of family size\n",
    "data['Single'] = data['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "data['SmallF'] = data['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "data['MedF'] = data['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "data['LargeF'] = data['Fsize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "\n",
    "# add Cabin feature\n",
    "for i, item in data['Cabin'].iteritems():\n",
    "    if not pd.isnull(item):\n",
    "        data.loc[i, 'Cabin'] = item[0]\n",
    "    else:\n",
    "        data.loc[i, 'Cabin'] = 'X'\n",
    "        \n",
    "## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n",
    "Ticket = []\n",
    "for i in list(data.Ticket):\n",
    "    if not i.isdigit() :\n",
    "        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0])\n",
    "    else:\n",
    "        Ticket.append(\"X\")\n",
    "        \n",
    "data['Ticket'] = Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Fsize</th>\n",
       "      <th>Single</th>\n",
       "      <th>SmallF</th>\n",
       "      <th>MedF</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_0</th>\n",
       "      <th>Title_1</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex   Age  SibSp  Parch     Fare  Fsize  Single  SmallF  MedF  \\\n",
       "0       0.0    0  22.0      1      0   7.2500      2       0       1     0   \n",
       "1       1.0    1  38.0      1      0  71.2833      2       0       1     0   \n",
       "2       1.0    1  26.0      0      0   7.9250      1       1       0     0   \n",
       "3       1.0    1  35.0      1      0  53.1000      2       0       1     0   \n",
       "\n",
       "   ...  Title_0  Title_1  Title_2  Title_3  Embarked_C  Embarked_Q  \\\n",
       "0  ...        0        0        1        0           0           0   \n",
       "1  ...        0        1        0        0           1           0   \n",
       "2  ...        0        1        0        0           0           0   \n",
       "3  ...        0        1        0        0           0           0   \n",
       "\n",
       "   Embarked_S  Pclass_1  Pclass_2  Pclass_3  \n",
       "0           1         0         0         1  \n",
       "1           0         1         0         0  \n",
       "2           1         0         0         1  \n",
       "3           1         1         0         0  \n",
       "\n",
       "[4 rows x 67 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['PassengerId', 'Name'], axis=1, inplace=True)\n",
    "data[\"Sex\"] = data[\"Sex\"].map({\"male\": 0, \"female\":1})\n",
    "data = pd.get_dummies(data, columns=['Cabin'], prefix='Cabin')\n",
    "data = pd.get_dummies(data, columns=['Ticket'], prefix='Ticket')\n",
    "data = pd.get_dummies(data, columns=['Title'], prefix='Title')\n",
    "data = pd.get_dummies(data, columns=[\"Embarked\"], prefix=\"Embarked\")\n",
    "data = pd.get_dummies(data, columns=[\"Pclass\"], prefix=\"Pclass\")\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import feature_selection \n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(titanic_train)\n",
    "X_train = data[:train_len].drop(['Survived'], axis=1)\n",
    "Y_train = data[:train_len]['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(grid_search):\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_clf = ensemble.RandomForestClassifier(n_estimators=50, max_features='sqrt')\n",
    "\n",
    "rf_param_grid = {\n",
    "              \"max_depth\": [None],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"criterion\": [\"gini\"]\n",
    "}\n",
    "\n",
    "rf_grid_search = model_selection.GridSearchCV(rf_clf, rf_param_grid, \n",
    "                                              scoring='accuracy', cv=10, n_jobs=8)\n",
    "rf_grid_search.fit(X_train, Y_train)\n",
    "rf_clf_best = rf_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "0.8308740068104427\n"
     ]
    }
   ],
   "source": [
    "show_result(rf_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  52 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=8)]: Done 280 out of 280 | elapsed:    8.8s finished\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier\n",
    "svm_clf = svm.SVC(probability=True)\n",
    "svc_param_grid = {'kernel': ['rbf'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100, 200, 300, 1000]}\n",
    "\n",
    "svm_grid_search= model_selection.GridSearchCV(SVM_clf, svc_param_grid, \n",
    "                                      cv=10, scoring=\"accuracy\", n_jobs=8, verbose=1)\n",
    "svm_grid_search.fit(X_train,Y_train)\n",
    "\n",
    "svm_clf_best = grid_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 200, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.8103254769921436\n"
     ]
    }
   ],
   "source": [
    "show_result(grid_search_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgbm = lightgbm.LGBMClassifier(silent=False)\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1], \n",
    "    'n_estimators':[100, 200, 300],\n",
    "    'max_depth': [4, 8, 15],\n",
    "    'min_samples_leaf': [100,150],\n",
    "    'max_features': [0.3, 0.1],\n",
    "    'early_stopping_rounds': [10]\n",
    "}\n",
    "\n",
    "lgbm_grid_search = model_selection.GridSearchCV(lgbm, param_grid, \n",
    "                                                scoring='accuracy', cv=10, n_jobs=8)\n",
    "lgbm_grid_search.fit(X_train, Y_train)\n",
    "\n",
    "lgbm_best = lgbm_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 100}\n",
      "0.8274687854710556\n"
     ]
    }
   ],
   "source": [
    "show_result(lgbm_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees\n",
    "ext_clf = ensemble.ExtraTreesClassifier()\n",
    "\n",
    "ext_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "ext_grid_search = model_selection.GridSearchCV(ext_clf, ext_param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv=10, n_jobs=8)\n",
    "ext_grid_search.fit(X_train, Y_train)\n",
    "ext_clf_best = ext_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "0.8297389330306469\n"
     ]
    }
   ],
   "source": [
    "show_result(ext_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('svm',\n",
       "                              SVC(C=200, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decision_function_shape='ovr',\n",
       "                                  degree=3, gamma=0.001, kernel='rbf',\n",
       "                                  max_iter=-1, probability=True,\n",
       "                                  random_state=None, shrinking=True, tol=0.001,\n",
       "                                  verbose=False)),\n",
       "                             ('lgbm',\n",
       "                              LGBMClassifier(boosting_type='gbdt',\n",
       "                                             class_weight=None,\n",
       "                                             colsample_bytree=1.0,\n",
       "                                             importance_type='split',\n",
       "                                             le...\n",
       "                                                   class_weight=None,\n",
       "                                                   criterion='gini',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features=3,\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=3,\n",
       "                                                   min_samples_split=10,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=300,\n",
       "                                                   n_jobs=None, oob_score=False,\n",
       "                                                   random_state=None, verbose=0,\n",
       "                                                   warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=8, voting='soft', weights=None)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = ensemble.VotingClassifier(estimators=[\n",
    "    ('svm', svm_clf_best), ('lgbm', lgbm_best), \n",
    "    ('rf', rf_clf_best), ('ext', ext_clf_best)], voting='soft', n_jobs=8)\n",
    "\n",
    "voting_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[train_len:].drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_survived = voting_clf.predict(test)\n",
    "# test_survived = rf_clf_best.predict(test)\n",
    "result = pd.concat([titanic_test['PassengerId'],\n",
    "                    pd.Series(test_survived, name='Survived', dtype='int')],\n",
    "                   axis=1)\n",
    "\n",
    "result.to_csv(\"ensemble_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randam Forest: 0.8626560726447219\n",
      "LightGBM: 0.960272417707151\n",
      "Extra Tree: 0.8535754824063564\n"
     ]
    }
   ],
   "source": [
    "# voting_clf.score(X_train, Y_train)\n",
    "# print('SVM:', svm_clf_best.score(X_train, Y_train))\n",
    "print('Randam Forest:', rf_clf_best.score(X_train, Y_train))\n",
    "print('LightGBM:', lgbm_best.score(X_train, Y_train))\n",
    "print('Extra Tree:', ext_clf_best.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_survived = model.predict(test)\n",
    "result = pd.concat([titanic_test['PassengerId'],\n",
    "                    pd.Series(test_survived, name='Survived', dtype='int')],\n",
    "                   axis=1)\n",
    "result.to_csv(\"rf_clf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8630751964085297"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
